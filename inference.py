"""
æ¨ç†å’Œæäº¤ç”Ÿæˆè…³æœ¬
Scientific Image Forgery Detection

ä½¿ç”¨æ–¹æ³•:
    python inference.py --checkpoint outputs/best_model.pth
    python inference.py --checkpoint outputs/best_model.pth --tta
"""

import os
import argparse
from pathlib import Path
from tqdm import tqdm
import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from PIL import Image
import cv2
import segmentation_models_pytorch as smp

from dataset import ForgeryDataset, get_val_transforms, get_tta_transforms
from utils import get_device, rle_encode


def parse_args():
    parser = argparse.ArgumentParser(description='Inference for Forgery Detection')
    
    # è·¯å¾‘
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='æ¨¡å‹æª¢æŸ¥é»è·¯å¾‘')
    parser.add_argument('--data_dir', type=str, default='./data',
                       help='è³‡æ–™ç›®éŒ„')
    parser.add_argument('--output_dir', type=str, default='./outputs',
                       help='è¼¸å‡ºç›®éŒ„')
    parser.add_argument('--submission_name', type=str, default='submission.csv',
                       help='æäº¤æª”æ¡ˆåç¨±')
    
    # æ¨ç†é…ç½®
    parser.add_argument('--batch_size', type=int, default=8,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--image_size', type=int, default=512,
                       help='è¼¸å…¥åœ–åƒå¤§å°')
    parser.add_argument('--threshold', type=float, default=0.5,
                       help='äºŒå€¼åŒ–é–¾å€¼')
    parser.add_argument('--tta', action='store_true',
                       help='ä½¿ç”¨ Test Time Augmentation')
    parser.add_argument('--save_masks', action='store_true',
                       help='ä¿å­˜é æ¸¬é®ç½©åœ–åƒ')
    
    return parser.parse_args()


def get_model_from_checkpoint(checkpoint_path, device):
    """å¾æª¢æŸ¥é»åŠ è¼‰æ¨¡å‹"""
    checkpoint = torch.load(checkpoint_path, map_location=device)
    args = checkpoint.get('args', {})
    
    # ç²å–æ¨¡å‹é…ç½®
    model_name = args.get('model', 'unet')
    encoder_name = args.get('encoder', 'resnet50')
    
    print(f"Loading {model_name} with {encoder_name} encoder...")
    
    # å‰µå»ºæ¨¡å‹
    model_dict = {
        'unet': smp.Unet,
        'unetpp': smp.UnetPlusPlus,
        'deeplabv3': smp.DeepLabV3,
        'deeplabv3p': smp.DeepLabV3Plus,
        'fpn': smp.FPN,
        'pspnet': smp.PSPNet,
        'manet': smp.MAnet,
    }
    
    model = model_dict[model_name](
        encoder_name=encoder_name,
        encoder_weights=None,
        in_channels=3,
        classes=1,
    )
    
    # åŠ è¼‰æ¬Šé‡
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()
    
    best_score = checkpoint.get('best_score', 0)
    print(f"Model loaded. Best validation score: {best_score:.4f}")
    
    return model, args


def inference_single(model, image, transform, device):
    """å–®å¼µåœ–åƒæ¨ç†"""
    # æ‡‰ç”¨è½‰æ›
    augmented = transform(image=image)
    img_tensor = augmented['image'].unsqueeze(0).to(device)
    
    # æ¨ç†
    with torch.no_grad():
        output = model(img_tensor)
        pred = torch.sigmoid(output)
    
    return pred.squeeze().cpu().numpy()


def inference_with_tta(model, image, tta_transforms, device):
    """å¸¶ TTA çš„æ¨ç†"""
    predictions = []
    
    for transform in tta_transforms:
        augmented = transform(image=image)
        img_tensor = augmented['image'].unsqueeze(0).to(device)
        
        with torch.no_grad():
            output = model(img_tensor)
            pred = torch.sigmoid(output).squeeze().cpu().numpy()
        
        predictions.append(pred)
    
    # åè½‰å¢å¼· (éœ€è¦å°é æ¸¬çµæœé€²è¡Œç›¸æ‡‰çš„åè®Šæ›)
    # é€™è£¡ç°¡åŒ–è™•ç†ï¼Œåªåšå¹³å‡
    # predictions[1] æ˜¯æ°´å¹³ç¿»è½‰ï¼Œéœ€è¦å†ç¿»è½‰å›ä¾†
    predictions[1] = np.fliplr(predictions[1])
    # predictions[2] æ˜¯å‚ç›´ç¿»è½‰
    predictions[2] = np.flipud(predictions[2])
    # predictions[3] æ˜¯è½‰ç½®
    predictions[3] = predictions[3].T
    
    # å¹³å‡
    final_pred = np.mean(predictions, axis=0)
    
    return final_pred


def resize_prediction(pred, original_size):
    """å°‡é æ¸¬èª¿æ•´å›åŸå§‹å¤§å°"""
    return cv2.resize(pred, (original_size[1], original_size[0]), 
                     interpolation=cv2.INTER_LINEAR)


def main():
    args = parse_args()
    
    # è¨­å‚™
    device = get_device()
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    if args.save_masks:
        mask_dir = output_dir / 'predicted_masks'
        mask_dir.mkdir(exist_ok=True)
    
    # åŠ è¼‰æ¨¡å‹
    print("\nğŸ”§ Loading model...")
    model, model_args = get_model_from_checkpoint(args.checkpoint, device)
    
    # è³‡æ–™è·¯å¾‘
    data_dir = Path(args.data_dir)
    test_image_dir = data_dir / 'test_images'
    sample_sub_path = data_dir / 'sample_submission.csv'
    
    # æª¢æŸ¥æ¸¬è©¦è³‡æ–™
    if not test_image_dir.exists():
        print(f"âŒ Test directory not found: {test_image_dir}")
        return
    
    # ç²å–æ‰€æœ‰æ¸¬è©¦åœ–åƒ
    test_images = sorted([
        f for f in test_image_dir.iterdir()
        if f.suffix.lower() in ['.png', '.jpg', '.jpeg', '.tif', '.tiff', '.bmp']
    ])
    print(f"\nğŸ“Š Found {len(test_images)} test images")
    
    # æº–å‚™è½‰æ›
    image_size = (args.image_size, args.image_size)
    transform = get_val_transforms(image_size)
    tta_transforms = get_tta_transforms(image_size) if args.tta else None
    
    # è®€å–æäº¤ç¯„ä¾‹ä»¥äº†è§£æ ¼å¼
    if sample_sub_path.exists():
        sample_df = pd.read_csv(sample_sub_path)
        print(f"\nSample submission format:")
        print(sample_df.head())
        submission_columns = sample_df.columns.tolist()
    else:
        print("\nâš ï¸ Sample submission not found, using default format")
        submission_columns = ['image_id', 'rle_mask']
    
    # æ¨ç†
    print(f"\nğŸš€ Running inference {'with TTA' if args.tta else ''}...")
    results = []
    
    for img_path in tqdm(test_images, desc='Inference'):
        # è®€å–åœ–åƒ
        image = cv2.imread(str(img_path))
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        original_size = image.shape[:2]  # (H, W)
        
        # æ¨ç†
        if args.tta:
            pred = inference_with_tta(model, image, tta_transforms, device)
        else:
            pred = inference_single(model, image, transform, device)
        
        # èª¿æ•´å›åŸå§‹å¤§å°
        pred_resized = resize_prediction(pred, original_size)
        
        # äºŒå€¼åŒ–
        pred_binary = (pred_resized > args.threshold).astype(np.uint8)
        
        # RLE ç·¨ç¢¼
        rle = rle_encode(pred_binary)
        
        # è¨˜éŒ„çµæœ
        results.append({
            'image_id': img_path.stem,
            'rle_mask': rle if rle else ''
        })
        
        # ä¿å­˜é®ç½©åœ–åƒ
        if args.save_masks:
            mask_save_path = mask_dir / f"{img_path.stem}.png"
            cv2.imwrite(str(mask_save_path), pred_binary * 255)
    
    # å‰µå»ºæäº¤æª”æ¡ˆ
    submission_df = pd.DataFrame(results)
    
    # ç¢ºä¿åˆ—ååŒ¹é… (å¯èƒ½éœ€è¦æ ¹æ“šå¯¦éš›æ¯”è³½èª¿æ•´)
    if 'image_id' not in submission_columns:
        # å˜—è©¦æ‰¾åˆ°æ­£ç¢ºçš„åˆ—å
        id_col = [c for c in submission_columns if 'id' in c.lower()]
        mask_col = [c for c in submission_columns if 'mask' in c.lower() or 'rle' in c.lower()]
        
        if id_col and mask_col:
            submission_df.columns = [id_col[0], mask_col[0]]
    
    # ä¿å­˜æäº¤æª”æ¡ˆ
    submission_path = output_dir / args.submission_name
    submission_df.to_csv(submission_path, index=False)
    
    print(f"\nâœ… Submission saved to: {submission_path}")
    print(f"   Total predictions: {len(submission_df)}")
    print(f"   Non-empty masks: {(submission_df.iloc[:, 1] != '').sum()}")
    
    # é¡¯ç¤ºå‰å¹¾å€‹çµæœ
    print("\nFirst 5 predictions:")
    print(submission_df.head())
    
    # çµ±è¨ˆ
    mask_sizes = []
    for rle in submission_df.iloc[:, 1]:
        if rle:
            # å¾ RLE è¨ˆç®—é®ç½©å¤§å°
            values = [int(x) for x in rle.split()[1::2]]
            mask_sizes.append(sum(values))
        else:
            mask_sizes.append(0)
    
    print(f"\nMask statistics:")
    print(f"   Mean size: {np.mean(mask_sizes):.0f} pixels")
    print(f"   Max size: {max(mask_sizes)} pixels")
    print(f"   Zero masks: {mask_sizes.count(0)}")


if __name__ == '__main__':
    main()
