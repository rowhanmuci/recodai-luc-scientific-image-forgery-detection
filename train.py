"""
è¨“ç·´è…³æœ¬
Scientific Image Forgery Detection

ä½¿ç”¨æ–¹æ³•:
    python train.py --model unet --encoder resnet50 --epochs 50
    python train.py --model unetpp --encoder efficientnet-b4 --loss focal_dice
"""

import os
import argparse
from pathlib import Path
from tqdm import tqdm
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau
import segmentation_models_pytorch as smp

from dataset import (
    ForgeryDataset, 
    get_train_transforms, 
    get_val_transforms,
    create_dataloaders
)
from losses import get_loss_function
from utils import (
    set_seed, get_device, AverageMeter, 
    batch_metrics, save_checkpoint, load_checkpoint,
    EarlyStopping, visualize_predictions, plot_training_history
)


def parse_args():
    parser = argparse.ArgumentParser(description='Train Forgery Detection Model')
    
    # è³‡æ–™è·¯å¾‘
    parser.add_argument('--data_dir', type=str, default='./data',
                       help='è³‡æ–™ç›®éŒ„è·¯å¾‘')
    parser.add_argument('--output_dir', type=str, default='./outputs',
                       help='è¼¸å‡ºç›®éŒ„')
    
    # æ¨¡å‹é…ç½®
    parser.add_argument('--model', type=str, default='unet',
                       choices=['unet', 'unetpp', 'deeplabv3', 'deeplabv3p', 'fpn', 'pspnet', 'manet'],
                       help='åˆ†å‰²æ¨¡å‹æ¶æ§‹')
    parser.add_argument('--encoder', type=str, default='resnet50',
                       help='Encoder backbone (resnet50, efficientnet-b4, etc.)')
    parser.add_argument('--pretrained', type=str, default='imagenet',
                       help='é è¨“ç·´æ¬Šé‡')
    
    # è¨“ç·´é…ç½®
    parser.add_argument('--epochs', type=int, default=50,
                       help='è¨“ç·´è¼ªæ•¸')
    parser.add_argument('--batch_size', type=int, default=8,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=1e-4,
                       help='å­¸ç¿’ç‡')
    parser.add_argument('--weight_decay', type=float, default=1e-4,
                       help='æ¬Šé‡è¡°æ¸›')
    parser.add_argument('--image_size', type=int, default=512,
                       help='è¼¸å…¥åœ–åƒå¤§å°')
    
    # æå¤±å‡½æ•¸
    parser.add_argument('--loss', type=str, default='bce_dice',
                       choices=['dice', 'focal', 'tversky', 'bce_dice', 'focal_dice', 'bce'],
                       help='æå¤±å‡½æ•¸')
    
    # å…¶ä»–
    parser.add_argument('--seed', type=int, default=42,
                       help='éš¨æ©Ÿç¨®å­')
    parser.add_argument('--num_workers', type=int, default=4,
                       help='DataLoader workers')
    parser.add_argument('--val_split', type=float, default=0.2,
                       help='é©—è­‰é›†æ¯”ä¾‹')
    parser.add_argument('--patience', type=int, default=10,
                       help='æ—©åœè€å¿ƒå€¼')
    parser.add_argument('--use_supplemental', action='store_true',
                       help='æ˜¯å¦ä½¿ç”¨è£œå……è³‡æ–™')
    parser.add_argument('--resume', type=str, default=None,
                       help='å¾æª¢æŸ¥é»æ¢å¾©è¨“ç·´')
    
    return parser.parse_args()


def get_model(model_name, encoder_name, pretrained='imagenet', in_channels=3, classes=1):
    """
    ç²å–åˆ†å‰²æ¨¡å‹
    
    å¯ç”¨æ¨¡å‹:
    - unet: U-Net
    - unetpp: U-Net++
    - deeplabv3: DeepLabV3
    - deeplabv3p: DeepLabV3+
    - fpn: Feature Pyramid Network
    - pspnet: Pyramid Scene Parsing Network
    - manet: Multi-scale Attention Net
    """
    model_dict = {
        'unet': smp.Unet,
        'unetpp': smp.UnetPlusPlus,
        'deeplabv3': smp.DeepLabV3,
        'deeplabv3p': smp.DeepLabV3Plus,
        'fpn': smp.FPN,
        'pspnet': smp.PSPNet,
        'manet': smp.MAnet,
    }
    
    if model_name not in model_dict:
        raise ValueError(f"Unknown model: {model_name}")
    
    model = model_dict[model_name](
        encoder_name=encoder_name,
        encoder_weights=pretrained,
        in_channels=in_channels,
        classes=classes,
    )
    
    return model


def train_one_epoch(model, loader, criterion, optimizer, device, scaler=None):
    """è¨“ç·´ä¸€å€‹ epoch"""
    model.train()
    losses = AverageMeter()
    
    pbar = tqdm(loader, desc='Training', leave=False)
    
    for batch in pbar:
        images = batch['image'].to(device)
        masks = batch['mask'].to(device)
        
        optimizer.zero_grad()
        
        # æ··åˆç²¾åº¦è¨“ç·´
        if scaler:
            with torch.cuda.amp.autocast():
                outputs = model(images)
                loss = criterion(outputs, masks)
            
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            outputs = model(images)
            loss = criterion(outputs, masks)
            loss.backward()
            optimizer.step()
        
        losses.update(loss.item(), images.size(0))
        pbar.set_postfix({'loss': f'{losses.avg:.4f}'})
    
    return losses.avg


@torch.no_grad()
def validate(model, loader, criterion, device):
    """é©—è­‰"""
    model.eval()
    losses = AverageMeter()
    all_metrics = {
        'precision': [], 'recall': [], 'f1': [],
        'iou': [], 'dice': [], 'accuracy': []
    }
    
    pbar = tqdm(loader, desc='Validation', leave=False)
    
    for batch in pbar:
        images = batch['image'].to(device)
        masks = batch['mask'].to(device)
        
        outputs = model(images)
        loss = criterion(outputs, masks)
        
        losses.update(loss.item(), images.size(0))
        
        # è¨ˆç®—æŒ‡æ¨™
        preds = torch.sigmoid(outputs)
        metrics = batch_metrics(preds, masks)
        
        for k, v in metrics.items():
            all_metrics[k].append(v)
        
        pbar.set_postfix({
            'loss': f'{losses.avg:.4f}',
            'dice': f'{np.mean(all_metrics["dice"]):.4f}'
        })
    
    # è¨ˆç®—å¹³å‡æŒ‡æ¨™
    avg_metrics = {k: np.mean(v) for k, v in all_metrics.items()}
    avg_metrics['loss'] = losses.avg
    
    return avg_metrics


def main():
    args = parse_args()
    
    # è¨­ç½®éš¨æ©Ÿç¨®å­
    set_seed(args.seed)
    
    # è¨­å‚™
    device = get_device()
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è³‡æ–™è·¯å¾‘ - æ”¯æ´æ–°çš„ç›®éŒ„çµæ§‹ (train_images/forged/)
    data_dir = Path(args.data_dir)
    
    # æª¢æŸ¥ç›®éŒ„çµæ§‹
    if (data_dir / 'train_images' / 'forged').exists():
        # æ–°çµæ§‹ï¼šåªç”¨ forged åœ–åƒè¨“ç·´åˆ†å‰²æ¨¡å‹
        train_image_dir = data_dir / 'train_images' / 'forged'
        print("ğŸ“‚ Using forged images only for segmentation training")
    else:
        # èˆŠçµæ§‹
        train_image_dir = data_dir / 'train_images'
    
    train_mask_dir = data_dir / 'train_masks'
    
    # å¦‚æœä½¿ç”¨è£œå……è³‡æ–™ï¼Œåˆä½µè·¯å¾‘
    if args.use_supplemental:
        supp_image_dir = data_dir / 'supplemental_images'
        supp_mask_dir = data_dir / 'supplemental_masks'
        print("Using supplemental data for training")
    
    # å‰µå»º DataLoader
    print("\nğŸ“¦ Creating DataLoaders...")
    image_size = (args.image_size, args.image_size)
    
    train_loader, val_loader = create_dataloaders(
        train_image_dir=train_image_dir,
        train_mask_dir=train_mask_dir,
        image_size=image_size,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        val_split=args.val_split
    )
    
    # å‰µå»ºæ¨¡å‹
    print(f"\nğŸ”§ Creating model: {args.model} with {args.encoder} encoder...")
    model = get_model(
        model_name=args.model,
        encoder_name=args.encoder,
        pretrained=args.pretrained
    )
    model = model.to(device)
    
    # è¨ˆç®—åƒæ•¸é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"   Total parameters: {total_params:,}")
    print(f"   Trainable parameters: {trainable_params:,}")
    
    # æå¤±å‡½æ•¸
    print(f"\nğŸ“‰ Loss function: {args.loss}")
    criterion = get_loss_function(args.loss)
    
    # å„ªåŒ–å™¨
    optimizer = optim.AdamW(
        model.parameters(),
        lr=args.lr,
        weight_decay=args.weight_decay
    )
    
    # å­¸ç¿’ç‡èª¿åº¦å™¨
    scheduler = CosineAnnealingWarmRestarts(
        optimizer,
        T_0=10,
        T_mult=2,
        eta_min=1e-6
    )
    
    # æ··åˆç²¾åº¦è¨“ç·´
    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None
    
    # æ—©åœ
    early_stopping = EarlyStopping(patience=args.patience, mode='max')
    
    # è¨“ç·´æ­·å²
    history = {
        'train_loss': [], 'val_loss': [],
        'val_dice': [], 'val_iou': [], 'val_f1': []
    }
    
    # å¾æª¢æŸ¥é»æ¢å¾©
    start_epoch = 0
    best_score = 0
    
    if args.resume:
        start_epoch, best_score = load_checkpoint(args.resume, model, optimizer)
    
    # é–‹å§‹è¨“ç·´
    print(f"\nğŸš€ Starting training for {args.epochs} epochs...")
    print("=" * 60)
    
    for epoch in range(start_epoch, args.epochs):
        print(f"\nEpoch {epoch+1}/{args.epochs}")
        print("-" * 40)
        
        # è¨“ç·´
        train_loss = train_one_epoch(
            model, train_loader, criterion, optimizer, device, scaler
        )
        
        # é©—è­‰
        val_metrics = validate(model, val_loader, criterion, device)
        
        # æ›´æ–°å­¸ç¿’ç‡
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        
        # è¨˜éŒ„æ­·å²
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_metrics['loss'])
        history['val_dice'].append(val_metrics['dice'])
        history['val_iou'].append(val_metrics['iou'])
        history['val_f1'].append(val_metrics['f1'])
        
        # æ‰“å°çµæœ
        print(f"Train Loss: {train_loss:.4f}")
        print(f"Val Loss: {val_metrics['loss']:.4f} | "
              f"Dice: {val_metrics['dice']:.4f} | "
              f"IoU: {val_metrics['iou']:.4f} | "
              f"F1: {val_metrics['f1']:.4f}")
        print(f"LR: {current_lr:.2e}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        current_score = val_metrics['dice']
        if current_score > best_score:
            best_score = current_score
            save_checkpoint({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_score': best_score,
                'args': vars(args)
            }, output_dir / 'best_model.pth')
            print(f"âœ… New best model! Dice: {best_score:.4f}")
        
        # å®šæœŸä¿å­˜æª¢æŸ¥é»
        if (epoch + 1) % 10 == 0:
            save_checkpoint({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_score': best_score,
                'args': vars(args)
            }, output_dir / f'checkpoint_epoch{epoch+1}.pth')
        
        # æ—©åœæª¢æŸ¥
        if early_stopping(current_score):
            print(f"\nâš ï¸ Early stopping triggered at epoch {epoch+1}")
            break
    
    # ä¿å­˜æœ€çµ‚æ¨¡å‹
    save_checkpoint({
        'epoch': args.epochs,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'best_score': best_score,
        'args': vars(args)
    }, output_dir / 'final_model.pth')
    
    # ç¹ªè£½è¨“ç·´æ­·å²
    plot_training_history(history, output_dir / 'training_history.png')
    
    print("\n" + "=" * 60)
    print(f"ğŸ‰ Training complete!")
    print(f"   Best Dice Score: {best_score:.4f}")
    print(f"   Models saved to: {output_dir}")
    print("=" * 60)


if __name__ == '__main__':
    main()